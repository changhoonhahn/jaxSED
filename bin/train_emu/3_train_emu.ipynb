{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44c1445f-1150-4587-93a7-c10c420561c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, random\n",
    "from jax.nn import sigmoid\n",
    "from jax.tree_util import tree_map\n",
    "\n",
    "import optax\n",
    "import functools\n",
    "\n",
    "import torch\n",
    "from torch.utils import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc010b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import util as U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bc757e",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca9e56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = []\n",
    "for i in range(10): \n",
    "    thetas.append(np.load(os.path.join(U.data_dir(), 'seds', 'modelb', 'train_sed.modelb.0.thetas_sps.npz'))['arr_0'])\n",
    "thetas = np.concatenate(thetas, axis=0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d633c5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pca = np.load(os.path.join(U.data_dir(), 'seds', 'modelb', 'train_sed.modelb.x_pca.w0.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57eab7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_collate(batch):\n",
    "    return tree_map(np.asarray, data.default_collate(batch))\n",
    "\n",
    "class NumpyLoader(data.DataLoader):\n",
    "    def __init__(self, dataset, batch_size=1,\n",
    "                shuffle=False, sampler=None,\n",
    "                batch_sampler=None, num_workers=0,\n",
    "                pin_memory=False, drop_last=False,\n",
    "                timeout=0, worker_init_fn=None):\n",
    "        super(self.__class__, self).__init__(dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            sampler=sampler,\n",
    "            batch_sampler=batch_sampler,\n",
    "            num_workers=num_workers,\n",
    "            collate_fn=numpy_collate,\n",
    "            pin_memory=pin_memory,\n",
    "            drop_last=drop_last,\n",
    "            timeout=timeout,\n",
    "            worker_init_fn=worker_init_fn)\n",
    "\n",
    "class FlattenAndCast(object):\n",
    "    def __call__(self, pic):\n",
    "        return np.ravel(np.array(pic, dtype=jnp.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9814b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = int(0.9*x_pca.shape[0])\n",
    "train_dataloader = NumpyLoader(data.TensorDataset(torch.tensor(thetas[:N_train]), torch.tensor(x_pca[:N_train])), batch_size=500)\n",
    "valid_dataloader = NumpyLoader(data.TensorDataset(torch.tensor(thetas[N_train:]), torch.tensor(x_pca[N_train:])), batch_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3722a834",
   "metadata": {},
   "source": [
    "# set up MLP in jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc9d0ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonlin_act(x, beta, gamma):\n",
    "    return (gamma + sigmoid(beta * x) * (1 - gamma)) * x\n",
    "\n",
    "def init_mlp_params(layer_sizes, scale=1e-2):\n",
    "    keys = random.split(random.PRNGKey(1), len(layer_sizes))\n",
    "\n",
    "    params = []\n",
    "    for i, key in zip(np.arange(len(layer_sizes)-2), keys): \n",
    "        m, n = layer_sizes[i], layer_sizes[i+1]\n",
    "        w_key, b_key, _a_key, _b_key = random.split(key, num=4)\n",
    "        params.append([scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,)), \n",
    "                      scale * random.normal(_a_key, (n,)), scale * random.normal(_b_key, (n,))])\n",
    "\n",
    "    m, n = layer_sizes[-2], layer_sizes[-1]        \n",
    "    w_key, b_key, _a_key, _b_key = random.split(keys[-1], num=4)\n",
    "    params.append([scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))])\n",
    "    return params\n",
    "\n",
    "@functools.partial(jax.vmap, in_axes=(None, 0))\n",
    "def forward(params, inputs):\n",
    "    activations = inputs\n",
    "    for w, b, beta, gamma in params[:-1]:\n",
    "        outputs = jnp.dot(w, activations) + b\n",
    "        activations = nonlin_act(outputs, beta, gamma) #relu(outputs)#\n",
    "\n",
    "    final_w, final_b = params[-1]\n",
    "    return jnp.dot(final_w, activations) + final_b\n",
    "\n",
    "def mse_loss(params, inputs, targets):\n",
    "    preds = forward(params, inputs)\n",
    "    return jnp.mean((preds - targets) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b01d5c",
   "metadata": {},
   "source": [
    "# train using `optax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f426ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update function\n",
    "@jit\n",
    "def update(params, opt_state, inputs, targets):\n",
    "    loss, grads = jax.value_and_grad(mse_loss)(params, inputs, targets)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d39ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-22 17:15:25.029102: W external/xla/xla/service/gpu/gemm_fusion_autotuner.cc:822] Compiling 53 configs for 2 fusions on a single thread.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3.2513184547424316, Valid Loss: 3.5068962574005127\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [thetas.shape[1], 128, 128, 128, x_pca.shape[1]]\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Initialize the MLP and optimizer\n",
    "params = init_mlp_params(layer_sizes)\n",
    "optimizer = optax.adam(learning_rate)\n",
    "\n",
    "# Initialize optimizer state\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "# Training loop\n",
    "train_loss, valid_loss = [], []\n",
    "for epoch in range(100):\n",
    "    epoch_loss = 0.0\n",
    "    for x, y in train_dataloader:        \n",
    "        params, opt_state, loss = update(params, opt_state, x, y)\n",
    "        epoch_loss += loss\n",
    "    epoch_loss /= len(train_dataloader)\n",
    "    train_loss.append(epoch_loss)\n",
    "    \n",
    "    _loss = 0\n",
    "    for x, y in valid_dataloader: \n",
    "        loss = mse_loss(params, x, y)\n",
    "        _loss += loss\n",
    "    valid_loss.append(_loss/len(valid_dataloader))\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {epoch_loss}, Valid Loss: {valid_loss[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a9b8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4038c115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c977b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax-gpu",
   "language": "python",
   "name": "jax-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
